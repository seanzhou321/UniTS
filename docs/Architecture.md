# Implementation Architecture

## Overview
The Implementation of this UniTS may be understood as a RESTful service. 
- The batch scripts in the ./scripts/ folder resemble the client calls. 
- torchrun is like the API
- run.py and run_pretrain.py are like the RESTful controllers, which parses the input arguments and calls the services/components to fulfill the requests.
- exp.exp_sup.py and exp.exp_pretrain.py are the services that executes the training and inference processes. 
- models.UniTS.py and models.UniTS_zeroshot.py are the components that implement the actual behavior logic of the model

##### Note: models.UniTS and models.UniTS_zeroshot shares 90% of the code. There are only small differences in the model initialization, pretraining, forecasting, classification, anomaly detection, and imputation. 

## Training Data Configuration
1. The training data is configured in a data_provider/*.yaml file, which specifies the directories of the datasets.
2. The configuration file was loaded into exp.exp_*.py
3. The data is then loaded into the models during the initialization.
### Implemented Task Names in the Models
1. long_term_forecast, or short_term_forecast, or forecast
2. classification
3. imputation
4. anomaly_detection
5. *pretrain* (does not appear in exp.exp_*.py)
### Type of Data Sources
Defined in data_provider/data_factory.py.data_dict. It is specified by the 'data' attribute in data_provider/*yaml files
1. MSL
2. PSM
3. SMAP
4. SMD
5. SWAT
6. UEA
7. gluonts
7. ETTh1
8. ETTh2
9. ETTm1
10. ETTm2
10. custom
## Handling Different Data File Format
1. data_provider/data_loader.py handles the parsing of specific data source
2. data_provider/data_factory.py loads data in exp.exp_*.py
## Labeling the training data
1. During pretraining done by exp.exp_pretraining.py, the training signals are from marking for long_term_forecast and masking for classification.
2. During supervised learning done by exp.exp_sup.py, the training signals are from provided y values for long_term_forecast, label for classification, and none for imputation and anomaly detection.
## How prompt is constructed? What can be included
Prompt is constructed in models.UniTS*.py.prepare_prompt(). The code is the same between the two implementations
The prompts tokens are dataset specific learnable embeddings. To learn the prompt for a dataset, i.e. prompt tuning, the pre-trained model weights are frozen. 
Refer to $4.1 in the [paper](https://arxiv.org/pdf/2403.00131).
1. forecast: prompt=prefix_prompt+x+function_prompt
2. Classification: prompt=prefix_prompt+x+functional_prompt
3. imputation: prompt=prefix_prompt+masked_x+mask_prompt
4. anomaly_detection: prompt=prefix_prompt+x
prefix_prompt is the prompt_token in the code, which is specific to a dataset name defined by dataset field in the yaml files. 
x is the sample token which is added by a learnable position embedding
## Where is the dataset specific prompt embedding persisted? Do users need to manage the persistence? How to load the embedding into the existing model?
## How the task is specified in the model
See details in the Task tokens section inside of $4.1 of the [paper](https://arxiv.org/pdf/2403.00131). 
1. The forecast tasks have token length equal to the forecast length. 
2. For classification, one token is appended to the end of the prompt to indicate the class. The class embeddings (e classes) are either trained or generated by averaging CLS tokens of training samples in each classes. 
3. For imputation, missing values are imputed using the gen tokens (details in Appendix C.2)
4. for anomaly detection, no CLS token (see details in Appendix C.2 of the paper)
## How UniTS handle variable time sequence lengths and variables?
1. Use attention head to handle the time and variable dimension separately.
2. Share same DyLinear weights for each variable.
3. Share the same weights in Time MHSA for different variables (from Claude)
4. Subsampling/bi-linear interpolation of the DyLinear weights to adapt to variable time sequence lengths.
5. Number of attention maps in Variable MHSA equals to the number of variates. (from Claude)
6. The projection matrices for the key, query, and value computations are shared across variables.
   (Answered by Claude.)
7. e_layers in the bash script is for the number of module blocks.
## What is the output from Time MHSA and what gets fed into Variable HMSA?
## How few-shot is trained? Do the model weights change?
## How zero-shot is trained?
## Is supervised fine tuning and few-shot learning the same in UniTS?



